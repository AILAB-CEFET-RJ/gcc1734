{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f982e636",
   "metadata": {},
   "source": [
    "# Actor-Critic Didático em NumPy\n",
    "\n",
    "Este notebook implementa uma versão **minimalista** do algoritmo Actor-Critic em Python puro, usando NumPy e Gymnasium. A ideia é mostrar passo a passo como o ator (política) e o crítico (função de valor) são atualizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80529c6",
   "metadata": {},
   "source": [
    "## 1. Importar dependências e criar ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac0ff2",
   "metadata": {},
   "source": [
    "## 2. Discretização do espaço de estados\n",
    "\n",
    "O CartPole tem estados contínuos. Para simplificar, vamos discretizar cada dimensão em poucos intervalos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7895fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função para discretizar observações contínuas em índices de estados\n",
    "def discretize(obs, bins=(6,12,6,12)):\n",
    "    cart_pos, cart_vel, pole_angle, pole_vel = obs\n",
    "    bounds = [(-2.4, 2.4), (-3.0, 3.0), (-0.21, 0.21), (-3.0, 3.0)]\n",
    "    ratios = [(obs[i] - bounds[i][0]) / (bounds[i][1]-bounds[i][0]) for i in range(4)]\n",
    "    new_obs = [int(min(bins[i]-1, max(0, int(rat*bins[i])))) for i,rat in enumerate(ratios)]\n",
    "    state = sum([new_obs[i]*np.prod(bins[:i]) for i in range(4)])\n",
    "    return state\n",
    "\n",
    "n_states = np.prod([6,12,6,12])\n",
    "n_actions = env.action_space.n\n",
    "print(\"Número de estados discretizados:\", n_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530889b4",
   "metadata": {},
   "source": [
    "## 3. Inicialização do Ator e Crítico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de394c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Política: probabilidades de ações por estado (softmax)\n",
    "policy = np.ones((n_states, n_actions)) / n_actions\n",
    "\n",
    "# Função de valor\n",
    "V = np.zeros(n_states)\n",
    "\n",
    "alpha = 0.1  # taxa do crítico\n",
    "beta = 0.1   # taxa do ator\n",
    "gamma = 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2e196",
   "metadata": {},
   "source": [
    "## 4. Função para escolher ação segundo a política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_action(state):\n",
    "    return np.random.choice(n_actions, p=policy[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86967576",
   "metadata": {},
   "source": [
    "## 5. Atualização do Ator e Crítico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c476c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update(state, action, reward, next_state, done):\n",
    "    # TD error\n",
    "    target = reward + (0 if done else gamma*V[next_state])\n",
    "    delta = target - V[state]\n",
    "    \n",
    "    # Atualiza crítico\n",
    "    V[state] += alpha * delta\n",
    "    \n",
    "    # Atualiza ator (gradiente de log-softmax)\n",
    "    probs = policy[state].copy()\n",
    "    grad = -probs\n",
    "    grad[action] += 1.0\n",
    "    policy[state] += beta * delta * grad\n",
    "    # Re-normaliza\n",
    "    policy[state] = np.maximum(policy[state], 1e-8)\n",
    "    policy[state] /= np.sum(policy[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef8ef9",
   "metadata": {},
   "source": [
    "## 6. Loop de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35376b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "episodes = 200\n",
    "rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = discretize(obs)\n",
    "    done = False\n",
    "    total = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        obs2, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = discretize(obs2)\n",
    "        \n",
    "        update(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        total += reward\n",
    "    \n",
    "    rewards.append(total)\n",
    "\n",
    "print(\"Recompensa média nos últimos 20 episódios:\", np.mean(rewards[-20:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1879295",
   "metadata": {},
   "source": [
    "## 7. Curva de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episódio\")\n",
    "plt.ylabel(\"Recompensa total\")\n",
    "plt.title(\"Actor-Critic em CartPole (discretizado)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
