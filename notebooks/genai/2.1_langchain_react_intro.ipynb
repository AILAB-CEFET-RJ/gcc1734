{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain \n",
    "\n",
    "Este noteboook ilustra os conceitos b√°sicos da biblioteca LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cb6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain and required dependencies (uncomment if running in a new environment)\n",
    "# !pip install langchain openai duckduckgo-search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5cc84",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "\n",
    "O conceito de **\"chat model\"** refere-se a modelos de linguagem que seguem o formato de **conversas estruturadas**.\n",
    "\n",
    "> Um **chat model** √© um modelo que recebe uma **sequ√™ncia de mensagens** com pap√©is expl√≠citos (`system`, `user`, `assistant`) e responde de forma contextualizada, como em um chat.\n",
    "\n",
    "Um chat model n√£o recebe apenas um `prompt`, mas sim uma **lista de mensagens** como esta:\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What is the capital of Brazil?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "O modelo usa esse hist√≥rico para gerar uma resposta como:\n",
    "\n",
    "```json\n",
    "{\"role\": \"assistant\", \"content\": \"The capital of Brazil is Bras√≠lia.\"}\n",
    "```\n",
    "\n",
    "Na LangChain, os chat models s√£o inst√¢ncias de classes como:\n",
    "\n",
    "  * `ChatOpenAI`\n",
    "  * `ChatOllama`\n",
    "  * `ChatAnthropic`\n",
    "\n",
    "Eles seguem a interface da classe base `BaseChatModel`, e podem ser usados em cadeias (`Chains`), ferramentas (`Tools`) ou agentes (`Agents`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8092a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get_llm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291c81f",
   "metadata": {},
   "source": [
    "## Tipos de mensagem\n",
    "\n",
    "Na LangChain, h√° tr√™s classes para representar mensagens em uma conversa. Essas classes estruturam a conversa para modelos de chat (como `ChatOpenAI` ou `ChatOllama`) e s√£o essenciais para manter **contexto, coer√™ncia e controle do comportamento** do agente.\n",
    "\n",
    "- `HumanMessage`: Representa uma **mensagem enviada pelo usu√°rio**. √â o que o modelo deve interpretar como entrada.\n",
    "\n",
    "```python\n",
    "    HumanMessage(content=\"What is the weather like today?\")\n",
    "```\n",
    "\n",
    "- `AIMessage`: Representa uma **mensagem gerada pelo modelo (LLM)**. √â usada para manter o hist√≥rico da conversa.\n",
    "\n",
    "```python\n",
    "    AIMessage(content=\"The weather today is sunny and warm.\")\n",
    "```\n",
    "\n",
    "- `SystemMessage`: Fornece **instru√ß√µes de configura√ß√£o ou contexto** para o modelo. Define o comportamento esperado.\n",
    "\n",
    "```python\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers concisely.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f518ba6",
   "metadata": {},
   "source": [
    "## M√©todo predict_messages\n",
    "\n",
    "Um m√©todo (fun√ß√£o) importante da interface de `BaseChatModel` √© `predict_messages`. Essa fun√ß√£o recebe uma sequ√™ncia de mensagens (user, system, assistant) e retorna a pr√≥xima mensagem do modelo, mantendo o formato conversacional.\n",
    "\n",
    "A sequ√™ncia correta de mensagens na lista para chamar `predict_messages` deve sempre finalizar com `HumanMessage`, conforme a seguir:\n",
    "\n",
    "```\n",
    "[\n",
    "    SystemMessage(...),       # configura o comportamento\n",
    "    HumanMessage(...),        # pergunta 1\n",
    "    AIMessage(...),           # resposta 1\n",
    "    HumanMessage(...),        # pergunta 2 ‚Üê o modelo vai responder a essa\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acf064",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d304abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI break up with the robot? \n",
      "\n",
      "... Because it said, \"I need some space... and a little less processing power!\" üòÑ \n",
      "\n",
      "---\n",
      "\n",
      "Would you like to hear another joke?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "response = llm.predict_messages([\n",
    "    HumanMessage(content=\"Tell me a joke about Artificial Intelligence.\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0073b",
   "metadata": {},
   "source": [
    "#### Exemplo 2\n",
    "\n",
    "No exemplo abaixo, a fun√ß√£o `predict_messages` retorna um objeto `AIMessage` com o conte√∫do gerado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dfab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Brazil is **Bras√≠lia**. \n",
      "\n",
      "It‚Äôs a fascinating city ‚Äì it was purpose-built as the capital in 1960! üòä \n",
      "\n",
      "Do you want to know anything more about Bras√≠lia or Brazil in general?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "\n",
    "response = llm.predict_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of Brazil?\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08c7db",
   "metadata": {},
   "source": [
    "#### Exemplo 3\n",
    "\n",
    "Neste exemplo:\n",
    "\n",
    "- O modelo v√™ o contexto inteiro da conversa.\n",
    "\n",
    "- Ele sabe que j√° falou sobre Marie Curie antes.\n",
    "\n",
    "- A resposta √† segunda pergunta pode ser mais precisa e direta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0881573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie actually won **two** Nobel Prizes! Here's a breakdown of what she won them for:\n",
      "\n",
      "*   **1903 Nobel Prize in Physics:** This was jointly awarded to Marie, her husband Pierre Curie, and Henri Becquerel. They won for their research into **radioactivity**. Specifically, they investigated the properties of uranium rays, which Marie coined the term \"radioactivity\" to describe.\n",
      "\n",
      "*   **1911 Nobel Prize in Chemistry:** This prize was awarded solely to Marie Curie for the discovery of the elements **polonium** and **radium**, and for isolating radium. This was a huge accomplishment ‚Äì she was the first person to win Nobel Prizes in two different scientific fields.\n",
      "\n",
      "\n",
      "It‚Äôs important to note that her work was incredibly difficult and dangerous, as she worked with radioactive materials without fully understanding the risks.\n",
      "\n",
      "Do you want me to delve deeper into any specific aspect of her work or her life?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# Hist√≥rico da conversa\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that explains things clearly.\"),\n",
    "    \n",
    "    HumanMessage(content=\"Who is Marie Curie?\"),\n",
    "    AIMessage(content=\"Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity. She was the first woman to win a Nobel Prize.\"),\n",
    "    \n",
    "    HumanMessage(content=\"What did she win the Nobel Prize for?\")\n",
    "]\n",
    "\n",
    "# O LLM ir√° responder √† √∫ltima pergunta considerando o hist√≥rico\n",
    "response = llm.predict_messages(messages)\n",
    "\n",
    "# Imprime a resposta gerada\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c5f30",
   "metadata": {},
   "source": [
    "Diferen√ßas entre chat model e LLM b√°sico:\n",
    "\n",
    "| Aspecto           | Chat Model                             | LLM B√°sico |\n",
    "| ----------------- | -------------------------------------- | ----------------------- |\n",
    "| Entrada           | Lista de mensagens (`role`, `content`) | Texto simples           |\n",
    "| Formato de prompt | Estruturado                            | Prompt plano            |\n",
    "| Exemplo t√≠pico    | ChatGPT, Claude, Gemini, Gemma         | Text-Davinci-003, GPT-J |\n",
    "| Classe base       | `BaseChatModel`                        | `BaseLLM`               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51863a17",
   "metadata": {},
   "source": [
    "## Cadeias (Chains)\n",
    "\n",
    "Em muitos casos simples, um chat model por si s√≥ √© suficiente. Mas o conceito de `Chain` na LangChain vai al√©m de apenas manter hist√≥rico: ele organiza a execu√ß√£o de etapas com reuso, modularidade e integra√ß√£o de m√∫ltiplos componentes.\n",
    "\n",
    "Por que usar apenas um chat model pode ser suficiente?\n",
    "\n",
    "Simplesmente isso:\n",
    "\n",
    "```python\n",
    "llm = get_llm()\n",
    "messages = [...]\n",
    "response = llm.predict_messages(messages)\n",
    "```\n",
    "\n",
    "resolve muitos casos ‚Äî principalmente quando:\n",
    "\n",
    "* Voc√™ **manualmente controla o hist√≥rico**\n",
    "* N√£o h√° necessidade de **pr√©-processamento**, **p√≥s-processamento**, ou **acesso a ferramentas**\n",
    "\n",
    "---\n",
    "\n",
    "Ent√£o, por que usar uma `Chain`? Porque em aplica√ß√µes reais, geralmente precisamos estruturar algo como:\n",
    "\n",
    "> Entrada ‚Üí preparar prompt ‚Üí gerar resposta ‚Üí extrair algo ‚Üí formatar sa√≠da ‚Üí logar/armazenar\n",
    "\n",
    "E isso pode envolver:\n",
    "\n",
    "* Integra√ß√£o com **retrievers**\n",
    "* Aplica√ß√£o de **mem√≥ria autom√°tica**\n",
    "* Uso de **ferramentas externas**\n",
    "* Extra√ß√£o de **informa√ß√µes estruturadas**\n",
    "* P√≥s-processamento da sa√≠da do LLM\n",
    "\n",
    "Comparando:\n",
    "\n",
    "| Situa√ß√£o                                   | Chat Model direto | Chain recomendada? |\n",
    "| ------------------------------------------ | ----------------- | ------------------ |\n",
    "| Simples conversa em linguagem natural      | ‚úÖ Sim             | ‚ùå N√£o precisa      |\n",
    "| Montagem din√¢mica de prompt                | ‚ö†Ô∏è Manual         | ‚úÖ Sim              |\n",
    "| Consulta a base de conhecimento (RAG)      | ‚ùå N√£o             | ‚úÖ Sim              |\n",
    "| M√∫ltiplos passos (ex: busca + sumariza√ß√£o) | ‚ùå N√£o             | ‚úÖ Sim              |\n",
    "| Logging, callbacks, parsing estruturado    | ‚ùå N√£o             | ‚úÖ Sim              |\n",
    "\n",
    "Analogia did√°tica:\n",
    "\n",
    "> Usar um chat model direto √© como usar uma **calculadora cient√≠fica**: √∫til e pontual.\n",
    "> Usar uma `Chain` √© como **programar uma planilha com v√°rias etapas**: voc√™ automatiza o processo, pode reutilizar partes, e conectar com outras fontes de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2e963",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common and natural translation of \"I love apples\" in French is:\n",
      "\n",
      "**J'aime les pommes.**\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **J'aime** - I love\n",
      "*   **les** - the (plural)\n",
      "*   **pommes** - apples\n",
      "\n",
      "You could also say:\n",
      "\n",
      "*   **Je suis amoureux/amoureuse des pommes.** (I am in love with apples - more emphatic) - *amoureux* is for a male speaker, *amoureuse* for a female.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# Composi√ß√£o de operadores\n",
    "chain = prompt | llm\n",
    "\n",
    "output = chain.invoke({\"text\": \"I love apples.\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feee070",
   "metadata": {},
   "source": [
    "#### Exemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26132bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. Cadeia de resumo\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize the following text:\\n\\n{text}\")\n",
    "summary_chain = summary_prompt | get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# 2. Cadeia de tradu√ß√£o\n",
    "translate_prompt = PromptTemplate.from_template(\"Translate to Portuguese:\\n\\n{text}\")\n",
    "translate_chain = translate_prompt | get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# 3. Cadeia composta\n",
    "def full_chain(article_text):\n",
    "    summary = summary_chain.invoke({\"text\": article_text})\n",
    "    translated = translate_chain.invoke({\"text\": summary})\n",
    "    return {\"summary\": summary, \"translated\": translated}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3975df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcac71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load get_llm.py\n",
    "def get_llm(model_backend, model_name=None):\n",
    "    if model_backend == \"openai\":\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            temperature=0,\n",
    "            model=model_name or \"gpt-3.5-turbo\"\n",
    "        )\n",
    "    elif model_backend == \"ollama\":\n",
    "        from langchain_ollama import ChatOllama\n",
    "        return ChatOllama(\n",
    "            temperature=0,\n",
    "            model=model_name or \"gemma3:latest\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backend: {model_backend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdbfae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between \"openai\" or \"ollama\"\n",
    "# MODEL_BACKEND = \"openai\"\n",
    "MODEL_BACKEND = \"ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c3dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "# Initialize the language model\n",
    "llm = get_llm(MODEL_BACKEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d38b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools the agent can use\n",
    "search = DuckDuckGoSearchRun()\n",
    "calculator = LLMMathChain.from_llm(llm)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"Useful for answering questions about current events or general knowledge.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=calculator.run,\n",
    "        description=\"Useful for solving mathematical problems.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9b1ad",
   "metadata": {},
   "source": [
    "Podemos inspecionar o prompt definido internamente na classe LLMMAthChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf38431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(calculator.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4018",
   "metadata": {},
   "source": [
    ">     agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION\n",
    "\n",
    "| Parte         | Significado                                                                                                                 |\n",
    "| ------------- | --------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `CHAT`        | Usa um modelo de chat (ex: `ChatOpenAI`, `ChatAnthropic`) que aceita mensagens estruturadas (`System`, `User`, `Assistant`) |\n",
    "| `ZERO_SHOT`   | N√£o requer exemplos (few-shot) para instruir o modelo ‚Äî apenas uma descri√ß√£o textual da tarefa e das ferramentas            |\n",
    "| `REACT`       | O agente segue o ciclo: **Thought ‚Üí Action ‚Üí Observation**                                                                  |\n",
    "| `DESCRIPTION` | As ferramentas s√£o descritas por texto, e o modelo escolhe qual usar com base nessas descri√ß√µes                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ba46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ReAct agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9864e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the sum of 144 and 25, and then find the square root of that sum.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"144+25\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 169\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to find the square root of 169.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"sqrt(169)\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 13.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 13\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'13'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run an example query\n",
    "agent.run(\"What is the square root of the sum of 144 and 25?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4d781",
   "metadata": {},
   "source": [
    "CHAT_ZERO_SHOT_REACT_DESCRIPTION x STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6bfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the square root of 81. This is a straightforward calculation.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"sqrt(81)\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 9.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 9\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "\n",
    "math_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=LLMMathChain.from_llm(llm).run,\n",
    "    description=\"Useful for solving math problems.\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[math_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What is the square root of 81?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103e2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The question asks for the square root of 81. I can use the calculator tool to perform this calculation.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": {\n",
      "    \"expression\": \"sqrt(81)\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sqrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      9\u001b[39m structured_math_tool = StructuredTool.from_function(\n\u001b[32m     10\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mCalculator\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mEvaluate a mathematical expression\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     func=sqrt_tool\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m agent = initialize_agent(\n\u001b[32m     16\u001b[39m     tools=[structured_math_tool],\n\u001b[32m     17\u001b[39m     llm=llm,\n\u001b[32m     18\u001b[39m     agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n\u001b[32m     19\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the square root of 81?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/chains/base.py:603\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m1\u001b[39m:\n\u001b[32m    602\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/agents/agent.py:1620\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1628\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1629\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1630\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/agents/agent.py:1328\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1319\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1324\u001b[39m ) -> Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m   1326\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1336\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/agents/agent.py:1411\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/agents/agent.py:1433\u001b[39m, in \u001b[36mAgentExecutor._perform_agent_action\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[39m\n\u001b[32m   1431\u001b[39m         tool_run_kwargs[\u001b[33m\"\u001b[39m\u001b[33mllm_prefix\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1432\u001b[39m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1433\u001b[39m     observation = \u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1434\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1441\u001b[39m     tool_run_kwargs = \u001b[38;5;28mself\u001b[39m._action_agent.tool_run_logging_kwargs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/tools/base.py:883\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    882\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    884\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    885\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/tools/base.py:852\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    850\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    851\u001b[39m         tool_kwargs = tool_kwargs | {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    854\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/tools/structured.py:93\u001b[39m, in \u001b[36mStructuredTool._run\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.func):\n\u001b[32m     92\u001b[39m         kwargs[config_param] = config\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mStructuredTool does not support sync invocation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36msqrt_tool\u001b[39m\u001b[34m(expression)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msqrt_tool\u001b[39m(expression: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'sqrt' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "def sqrt_tool(expression: str) -> str:\n",
    "    return str(eval(expression))\n",
    "\n",
    "structured_math_tool = StructuredTool.from_function(\n",
    "    name=\"Calculator\",\n",
    "    description=\"Evaluate a mathematical expression\",\n",
    "    func=sqrt_tool\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[structured_math_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What is the square root of 81?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcc1734",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
