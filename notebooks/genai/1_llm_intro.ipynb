{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fc61af",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e5acc",
   "metadata": {},
   "source": [
    "\n",
    "> A **language model** is a computational system, typically implemented using machine learning (especially neural networks), that is trained to **predict and generate natural language text** by learning statistical patterns from large corpora ‚Äî rather than being explicitly programmed with linguistic rules.\n",
    "\n",
    "> A machine learning-based **language model** is not hand-coded with grammar rules. Instead, it‚Äôs a type of machine learning model that learns how people use language by reading huge amounts of text. After training, it can complete sentences, answer questions, or write paragraphs ‚Äî just by predicting what words are likely to come next.\n",
    "\n",
    "> A language model is like an **auto-complete on steroids** ‚Äî trained on a huge amount of text, it can generate whole sentences, answer questions, write stories, or translate languages.\n",
    "\n",
    "### Think of it like this:\n",
    "\n",
    "* You start typing a message: ‚ÄúI am going to the‚Ä¶‚Äù\n",
    "* A language model can guess what comes next: ‚Äústore‚Äù, ‚Äúgym‚Äù, or ‚Äúbeach‚Äù.\n",
    "* It does this by learning patterns in how words are used together ‚Äî like how we humans learn language by reading and listening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf5a9f",
   "metadata": {},
   "source": [
    "Here‚Äôs a simple example in Python using the `transformers` library from Hugging Face. This code shows how to use a **pre-trained language model** to generate text:\n",
    "\n",
    "### üß† Example: Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a89c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebezerra/anaconda3/envs/gcc1734/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a small village, a group of men came together and started talking. This was not unusual because of the fact that villages are usually inhabited by children, so they had a tradition of talking. The man from the village that spoke was only six years old.\n",
      "\n",
      "\"We have a small group called the villagers, which are the first to go out into the forest. One of them has a small sword. He has a small sword, so I will fight against him.\"\n",
      "\n",
      "\"I will fight against the man, so you will get the sword. My name is Nii-san.\"\n",
      "\n",
      "Nii-san was the eldest daughter of the village, and she brought the sword she had learned from the village's teacher as a gift to a man. The sword she had used was a dagger.\n",
      "\n",
      "\"This sword will come in one piece, but it's not as sharp as the sword in the other. This sword can also be used as a weapon. The only thing you need to do is to prepare for battle. I won't be too nervous, but I did put it on a piece of paper.\"\n",
      "\n",
      "\"A piece?\"\n",
      "\n",
      "Nii-san nodded her head and spoke.\n",
      "\n",
      "\"I will say this once. First, I want\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained language model (GPT-2)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Give the model a prompt\n",
    "prompt = \"Once upon a time in a small village\"\n",
    "\n",
    "# Generate text\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print the result\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff737d7",
   "metadata": {},
   "source": [
    "### What it does:\n",
    "\n",
    "* Loads **GPT-2**, a popular language model.\n",
    "* Takes your prompt `\"Once upon a time in a small village\"`.\n",
    "* Predicts and adds more text, word by word.\n",
    "\n",
    "You‚Äôll get something like:\n",
    "\n",
    "```\n",
    "\"Once upon a time in a small village, there was a curious fox who loved to explore the nearby forest. Every morning...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a27322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcc1734",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
