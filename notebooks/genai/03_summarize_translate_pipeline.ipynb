{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613a88da",
   "metadata": {},
   "source": [
    "# LangChain - demo\n",
    "\n",
    "Este notebook demonstra como construir um pipeline reutilizável com LangChain que:\n",
    "\n",
    "1. Resume um texto em inglês\n",
    "2. Traduz o resumo para o português utilizando a interface `Runnable` do LangChain e a composição de prompts.\n",
    "\n",
    "Essa demonstração ilustra os seguintes conceitos e classes do LangChain:\n",
    "\n",
    "**Conceitos fundamentais**:\n",
    "\n",
    "1. **Prompting**\n",
    "   – Como estruturar entradas para modelos LLM de forma controlada e reutilizável.\n",
    "\n",
    "2. **Runnable pipeline**\n",
    "   – Composição modular de etapas de processamento usando `|`.\n",
    "\n",
    "3. **Encadeamento de componentes (`pipe`)**\n",
    "   – Conectar prompt → modelo → parser de forma declarativa.\n",
    "\n",
    "4. **Execução síncrona com `.invoke()`**\n",
    "   – Chamada direta de um pipeline com entrada como dicionário.\n",
    "\n",
    "**Classes do LangChain**:\n",
    "\n",
    "| Classe                      | Finalidade                                                                     |\n",
    "| --------------------------- | ------------------------------------------------------------------------------ |\n",
    "| `PromptTemplate`            | Cria e formata prompts com variáveis dinâmicas.                                |\n",
    "| `StrOutputParser`           | Limpa e interpreta a saída do modelo como string.                              |\n",
    "| `Runnable` (interface)      | Representa componentes executáveis (`.invoke()`), como prompts, LLMs, parsers. |\n",
    "| `ChatOpenAI` / `ChatOllama` | Interfaces para comunicação com modelos de linguagem (OpenAI ou Ollama).       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976442ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get_llm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumo: content='Marie Curie was a groundbreaking physicist and chemist who achieved unprecedented recognition, becoming the first woman and only individual to win Nobel Prizes in both Physics and Chemistry.' additional_kwargs={} response_metadata={'model': 'gemma3:latest', 'created_at': '2025-07-01T12:14:49.924704006Z', 'done': True, 'done_reason': 'stop', 'total_duration': 16838705396, 'load_duration': 10590350713, 'prompt_eval_count': 64, 'prompt_eval_duration': 2746761645, 'eval_count': 31, 'eval_duration': 3492347382, 'model_name': 'gemma3:latest'} id='run--ec8f2d1a-aaea-4554-902f-694839498737-0' usage_metadata={'input_tokens': 64, 'output_tokens': 31, 'total_tokens': 95}\n",
      "Tradução: content='Aqui está a tradução do resumo para o português:\\n\\n\"Marie Curie foi uma física e química inovadora que alcançou reconhecimento sem precedentes, tornando-se a primeira mulher e a única pessoa a ganhar Prêmios Nobel em ambas as áreas da Física e da Química.\"\\n\\n**Informações Adicionais (metadata):**\\n\\n*   **ID da Execução:** run--ec8f2d1a-aaea-4554-902f-694839498737-0\\n*   **Modelo:** gemma3:latest\\n*   **Data de Criação:** 2025-07-01T12:14:49.924704006Z\\n*   **Concluído:** Sim\\n*   **Motivo da Conclusão:** stop\\n*   **Duração Total:** 16838705396\\n*   **Duração de Carregamento:** 10590350713\\n*   **Contagem de Avaliação do Prompt:** 64\\n*   **Duração da Avaliação do Prompt:** 2746761645\\n*   **Contagem de Avaliação:** 31\\n*   **Duração da Avaliação:** 3492347382\\n*   **Nome do Modelo:** gemma3:latest\\n*   **Tokens de Entrada:** 64\\n*   **Tokens de Saída:** 31\\n*   **Tokens Totais:** 95' additional_kwargs={} response_metadata={'model': 'gemma3:latest', 'created_at': '2025-07-01T12:15:39.787637634Z', 'done': True, 'done_reason': 'stop', 'total_duration': 48457905824, 'load_duration': 93848718, 'prompt_eval_count': 293, 'prompt_eval_duration': 6804544241, 'eval_count': 348, 'eval_duration': 41557636773, 'model_name': 'gemma3:latest'} id='run--52986f32-5184-4e21-8423-60ac17b3dc29-0' usage_metadata={'input_tokens': 293, 'output_tokens': 348, 'total_tokens': 641}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# Prompt de sumarização\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"Resuma o seguinte texto em uma frase:\\n\\n{texto}\"\n",
    ")\n",
    "\n",
    "# Prompt de tradução\n",
    "translate_prompt = PromptTemplate.from_template(\n",
    "    \"Traduza para o português o seguinte resumo:\\n\\n{resumo}\"\n",
    ")\n",
    "\n",
    "# Instancia o modelo (usando sua função já definida)\n",
    "llm = get_llm()\n",
    "\n",
    "# Composição das chains\n",
    "summary_chain: Runnable = summary_prompt | llm \n",
    "translate_chain: Runnable = translate_prompt | llm\n",
    "\n",
    "# Entrada original\n",
    "artigo = \"\"\"\n",
    "Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity.\n",
    "She achieved unparalleled recognition, becoming the first woman to win a Nobel Prize\n",
    "and the only person to win Nobel Prizes in both Physics and Chemistry.\n",
    "\"\"\"\n",
    "\n",
    "# Execução\n",
    "resumo = summary_chain.invoke({\"texto\": artigo})\n",
    "traducao = translate_chain.invoke({\"resumo\": resumo})\n",
    "\n",
    "# Resultado final\n",
    "print(\"Resumo:\", resumo)\n",
    "print(\"Tradução:\", traducao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd6418",
   "metadata": {},
   "source": [
    "# Descrição do código\n",
    "\n",
    "## 1. Definição dos prompts\n",
    "\n",
    "A classe `PromptTemplate` do LangChain serve para criar prompts dinâmicos e reutilizáveis. Ela permite definir um template com variáveis que serão preenchidas em tempo de execução, facilitando a construção de mensagens para modelos LLM.\n",
    "\n",
    "> Propósito principal: separar a **estrutura do prompt** dos **dados de entrada**, permitindo reutilização e organização do código.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "```python\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "    template = PromptTemplate.from_template(\"Traduza para o português: {frase}\")\n",
    "    prompt = template.format(frase=\"Good morning!\")\n",
    "\n",
    "    print(prompt)\n",
    "    # Saída: \"Traduza para o português: Good morning!\"\n",
    "```\n",
    "\n",
    "Vantagens:\n",
    "\n",
    "* Facilita a **engenharia de prompts**\n",
    "* Reduz repetição de código\n",
    "* Integra bem com chains (`Runnable`) e LLMs\n",
    "\n",
    "## 2. Construção das cadeias de execução\n",
    "\n",
    "As linhas abaixo constroem cadeias de execução (chamadas *chains*) que conectam um prompt formatado a um modelo de linguagem (llm), usando o operador | (pipe).\n",
    "\n",
    "\n",
    "```python\n",
    "    summary_chain = summary_prompt | llm\n",
    "    translate_chain = translate_prompt | llm\n",
    "```\n",
    "\n",
    "Significado de cada parte:\n",
    "\n",
    "* `summary_prompt` e `translate_prompt`: objetos `PromptTemplate` que definem o *formato* do prompt para sumarização ou tradução, respectivamente.\n",
    "* `llm`: o modelo de linguagem obtido via sua função `get_llm()`.\n",
    "* `|`: operador de **encadeamento** (pipe), que conecta etapas compatíveis do LangChain.\n",
    "\n",
    "\n",
    "Resultado:\n",
    "\n",
    "* `summary_chain` é um *runnable* que:\n",
    "\n",
    "  1. Recebe dados como `{\"texto\": ...}`\n",
    "  2. Formata o prompt com `summary_prompt`\n",
    "  3. Envia o prompt ao modelo `llm`\n",
    "  4. Retorna a resposta\n",
    "\n",
    "* `translate_chain` funciona da mesma forma, mas com outro prompt.\n",
    "\n",
    "Utilidade das cadeias:\n",
    "\n",
    "* Permitem compor **pipelines modulares** de forma declarativa\n",
    "* Melhoram legibilidade e organização do código\n",
    "\n",
    "## 3. Execução das cadeias\n",
    "\n",
    "```python\n",
    "resumo = summary_chain.invoke({\"texto\": artigo})\n",
    "traducao = translate_chain.invoke({\"resumo\": resumo})\n",
    "```\n",
    "\n",
    "Essas duas linhas são a execução propriamente dita das chains definidas anteriormente com os prompts e o modelo (`summary_chain` e `translate_chain`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586dc22",
   "metadata": {},
   "source": [
    "# Descrição do resultado\n",
    "\n",
    "O resultado acima apresenta a resposta do modelo LLM em LangChain com metadados detalhados, incluindo:\n",
    "\n",
    "1. Conteúdo principal (tradução gerada e justificativas)\n",
    "2. Informações sobre desempenho (tokens, tempos)\n",
    "3. Cronometragem da execução no Jupyter\n",
    "\n",
    "\n",
    "## 1. Conteúdo principal\n",
    "\n",
    "O primeiro trecho do resultado mostra que o modelo traduziu o parágrafo para o português e adicionou comentários explicativos sobre suas escolhas de tradução.\n",
    "\n",
    "```text\n",
    "content=\"Here's the Portuguese translation of the paragraph:\n",
    "\n",
    "“Marie Curie foi uma física e química inovadora, conhecida por seu trabalho pioneiro sobre radioatividade. Ela alcançou reconhecimento sem precedentes, tornando-se a primeira mulher a ganhar um Prêmio Nobel e o único indivíduo a receber prêmios Nobel em Física e Química, graças às suas descobertas dos elementos polônio e rádio.”\n",
    "\n",
    "---\n",
    "\n",
    "**Notes on the translation:**\n",
    "\n",
    "*   I’ve aimed for a natural and accurate translation, maintaining the tone and information of the original English.\n",
    "*   “Groundbreaking” was translated as “inovadora” (innovative).\n",
    "*   “Unparalleled recognition” was translated as “reconhecimento sem precedentes” (unprecedented recognition).\n",
    "*   I’ve kept the names of the elements (polonium and radium) as they are commonly used in Portuguese as well.\"\n",
    "```\n",
    "\n",
    "## 2. Metadados da resposta\n",
    "\n",
    "```python\n",
    "response_metadata={\n",
    "  'model': 'gemma3:latest',\n",
    "  'created_at': '2025-07-01T10:42:05.597910801Z',\n",
    "  'done': True,\n",
    "  'done_reason': 'stop',\n",
    "  'total_duration': 31575438389,\n",
    "  'load_duration': 87887005,\n",
    "  'prompt_eval_count': 321,\n",
    "  'prompt_eval_duration': 7540430110,\n",
    "  'eval_count': 187,\n",
    "  'eval_duration': 23946329334,\n",
    "  'model_name': 'gemma3:latest'\n",
    "}\n",
    "```\n",
    "\n",
    "| Campo                  | Significado                                                                                                                            |\n",
    "| ---------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `model` / `model_name` | Nome do modelo utilizado. Neste caso, o modelo local `gemma3:latest` via Ollama.                                                       |\n",
    "| `created_at`           | Timestamp ISO 8601 da execução da inferência. Útil para logs e auditoria.                                                              |\n",
    "| `done`                 | Booleano indicando se a inferência terminou com sucesso.                                                                               |\n",
    "| `done_reason`          | Razão pela qual a geração foi encerrada — normalmente `\"stop\"` (fim natural), mas pode ser `\"length\"` (limite de tokens) ou `\"error\"`. |\n",
    "| `total_duration`       | Tempo total da chamada, em **nanosegundos**: `31.6 segundos`. Engloba todo o ciclo de inferência.                                      |\n",
    "| `load_duration`        | Tempo gasto para carregar ou inicializar o modelo no runtime: \\~88 ms.                                                                 |\n",
    "| `prompt_eval_count`    | Tokens no **prompt de entrada** (321 tokens), equivalente a `input_tokens`.                                                            |\n",
    "| `prompt_eval_duration` | Tempo gasto processando o prompt: \\~7.5 s.                                                                                             |\n",
    "| `eval_count`           | Tokens gerados na **resposta do modelo** (187 tokens), equivalente a `output_tokens`.                                                  |\n",
    "| `eval_duration`        | Tempo de geração da resposta: \\~23.9 s.                                                                                                |\n",
    "\n",
    "Utilidade prática dos metadados acima:\n",
    "\n",
    "- Ajudam a diagnosticar gargalos (ex: modelo lento, prompt muito longo). Permitem análise de desempenho por modelo, útil para comparar, por exemplo, gemma3 vs llama3.\n",
    "\n",
    "- Facilitam log e rastreabilidade de execuções em pipelines LangChain.\n",
    "\n",
    "\n",
    "\n",
    "## 3. Metadados de uso (do LLM)\n",
    "\n",
    "```python\n",
    "usage_metadata={\n",
    "  'input_tokens': 321,\n",
    "  'output_tokens': 187,\n",
    "  'total_tokens': 508\n",
    "}\n",
    "```\n",
    "\n",
    "| Campo           | Significado                                                       |\n",
    "| --------------- | ----------------------------------------------------------------- |\n",
    "| `input_tokens`  | Quantidade de tokens no prompt fornecido ao modelo (`321` tokens) |\n",
    "| `output_tokens` | Tokens gerados como resposta pelo modelo (`187` tokens)           |\n",
    "| `total_tokens`  | Soma total: entrada + saída = `321 + 187 = 508` tokens            |\n",
    "\n",
    "\n",
    "Esses números podem ser úteis para monitoramento e controle de uso.\n",
    "\n",
    "- Em modelos pagos (OpenAI , Gemini, Claude), usados para calcular custos em dólares.\n",
    "\n",
    "- Em modelos locais (ex: Ollama), servem para medir tempo de inferência, tamanho de contexto e desempenho.\n",
    "\n",
    "\n",
    "## 4. Identificador do run\n",
    "\n",
    "```python\n",
    "id='run--a25e68f1-e8a3-4a51-8e44-05e32ac4b3d0-0'\n",
    "```\n",
    "\n",
    "Esse é um ID único do `Runnable` ou da `chain.invoke()` executada, útil para debugging e rastreamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d56169",
   "metadata": {},
   "source": [
    "# Exemplo melhorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# Prompt de sumarização\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"Resuma o seguinte texto em uma frase:\\n\\n{texto}\"\n",
    ")\n",
    "\n",
    "# Prompt de tradução\n",
    "translate_prompt = PromptTemplate.from_template(\n",
    "    \"Traduza para o português o seguinte resumo:\\n\\n{resumo}\"\n",
    ")\n",
    "\n",
    "# Instancia o modelo (usando sua função já definida)\n",
    "llm = get_llm()\n",
    "\n",
    "# Parser para limpar a saída do modelo\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Composição das chains\n",
    "summary_chain: Runnable = summary_prompt | llm | parser\n",
    "translate_chain: Runnable = translate_prompt | llm | parser\n",
    "\n",
    "# Entrada original\n",
    "artigo = \"\"\"\n",
    "Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity.\n",
    "She achieved unparalleled recognition, becoming the first woman to win a Nobel Prize\n",
    "and the only person to win Nobel Prizes in both Physics and Chemistry.\n",
    "\"\"\"\n",
    "\n",
    "# Execução\n",
    "resumo = summary_chain.invoke({\"texto\": artigo})\n",
    "traducao = translate_chain.invoke({\"resumo\": resumo})\n",
    "\n",
    "# Resultado final\n",
    "print(\"Resumo:\", resumo)\n",
    "print(\"Tradução:\", traducao)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcc1734",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
