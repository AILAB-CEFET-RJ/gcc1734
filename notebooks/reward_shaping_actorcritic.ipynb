{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b8b9a5",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Reward Shaping and Actor-Critic\n",
    "\n",
    "Este notebook mostra dois exemplos didáticos:\n",
    "1. **Reward Shaping** em Gridworld com implementação em NumPy.\n",
    "2. **Actor-Critic (A2C)** usando Stable-Baselines3 em CartPole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54933261",
   "metadata": {},
   "source": [
    "## Parte 1: Reward Shaping em Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf77a80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aprendida (melhor ação por estado):\n",
      "[[0 1 1 2]\n",
      " [1 1 1 2]\n",
      " [1 1 1 2]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Gridworld simples 4x4\n",
    "n_states = 16\n",
    "n_actions = 4  # up, right, down, left\n",
    "gamma = 0.9\n",
    "\n",
    "# Função para converter estado em (linha,coluna)\n",
    "def to_pos(s):\n",
    "    return divmod(s, 4)\n",
    "\n",
    "# Movimentos\n",
    "actions = {0: (-1,0), 1: (0,1), 2: (1,0), 3: (0,-1)}\n",
    "\n",
    "# Estado terminal\n",
    "terminal = [0, 15]\n",
    "\n",
    "def step(s, a):\n",
    "    if s in terminal:\n",
    "        return s, 0\n",
    "    r,c = to_pos(s)\n",
    "    dr,dc = actions[a]\n",
    "    r2, c2 = max(0,min(3,r+dr)), max(0,min(3,c+dc))\n",
    "    s2 = 4*r2+c2\n",
    "    reward = -1\n",
    "    if s2==15: reward = 10\n",
    "    return s2, reward\n",
    "\n",
    "# Reward shaping potencial\n",
    "phi = np.array([-(abs(r-3)+abs(c-3)) for r in range(4) for c in range(4)])\n",
    "\n",
    "def shaping(s, s2):\n",
    "    return gamma*phi[s2]-phi[s]\n",
    "\n",
    "# Q-learning com reward shaping\n",
    "alpha=0.1\n",
    "eps=0.1\n",
    "episodes=500\n",
    "\n",
    "Q = np.zeros((n_states,n_actions))\n",
    "\n",
    "for ep in range(episodes):\n",
    "    s = np.random.randint(n_states)\n",
    "    while s not in terminal:\n",
    "        if np.random.rand()<eps:\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(Q[s])\n",
    "        s2, r = step(s,a)\n",
    "        r_shaped = r + shaping(s,s2)\n",
    "        Q[s,a] += alpha*(r_shaped + gamma*np.max(Q[s2]) - Q[s,a])\n",
    "        s=s2\n",
    "\n",
    "print(\"Política aprendida (melhor ação por estado):\")\n",
    "policy = np.array([np.argmax(Q[s]) for s in range(n_states)]).reshape(4,4)\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffafc0",
   "metadata": {},
   "source": [
    "A tabela acima mostra as melhores ações aprendidas em cada célula (0=up,1=right,2=down,3=left)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482b3f3",
   "metadata": {},
   "source": [
    "## Parte 2: Actor-Critic (A2C) em CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f471de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stable-baselines3 gymnasium[classic-control] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff43c1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebezerra/anaconda3/envs/gcc1734/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensas médias em 10 episódios: 203.6\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Criar ambiente CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Treinar A2C\n",
    "model = A2C(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Avaliar\n",
    "obs, info = env.reset()\n",
    "rewards = []\n",
    "for _ in range(10):\n",
    "    done=False\n",
    "    total=0\n",
    "    obs, info = env.reset()\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total += reward\n",
    "        done = terminated or truncated\n",
    "    rewards.append(total)\n",
    "\n",
    "print(\"Recompensas médias em 10 episódios:\", np.mean(rewards))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd10a81",
   "metadata": {},
   "source": [
    "O modelo A2C deve alcançar uma boa pontuação média após poucas iterações, demonstrando a efetividade dos métodos actor-critic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcc1734",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
