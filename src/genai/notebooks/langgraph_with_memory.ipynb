{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# LangGraph with Memory\n", "\n", "In this notebook, we demonstrate how to integrate memory into a LangGraph-based agent. This allows the agent to maintain context across steps and reuse past information."]}, {"cell_type": "code", "metadata": {}, "source": ["# !pip install langgraph langchain openai"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["from langgraph.graph import StateGraph, END\n", "from langchain.chat_models import ChatOpenAI\n", "from langchain.memory import ConversationBufferMemory\n", "\n", "# Initialize components\n", "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n", "memory = ConversationBufferMemory(return_messages=True)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Define Nodes\n", "- The `contextual_executor` will access memory before answering.\n", "- The `recorder` will store interactions in memory."]}, {"cell_type": "code", "metadata": {}, "source": ["# Executor with memory usage\n", "def contextual_executor(state):\n", "    memory.load_memory_variables({})  # Load memory\n", "    user_input = state.get(\"input\", \"\")\n", "    response = llm.predict(f\"Context: {memory.buffer}\\nUser: {user_input}\")\n", "    return {\"input\": user_input, \"response\": response}"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# Recorder stores the new interaction\n", "def recorder(state):\n", "    memory.save_context({\"input\": state[\"input\"]}, {\"output\": state[\"response\"]})\n", "    return state"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Build the graph"]}, {"cell_type": "code", "metadata": {}, "source": ["graph = StateGraph()\n", "graph.add_node(\"contextual_executor\", contextual_executor)\n", "graph.add_node(\"recorder\", recorder)\n", "\n", "graph.set_entry_point(\"contextual_executor\")\n", "graph.add_edge(\"contextual_executor\", \"recorder\")\n", "graph.add_edge(\"recorder\", END)\n", "\n", "app = graph.compile()"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Run the agent"]}, {"cell_type": "code", "metadata": {}, "source": ["# Example 1\n", "app.invoke({\"input\": \"What is 2 + 2?\"})"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["# Example 2: builds on memory\n", "app.invoke({\"input\": \"And what if we subtract 1 from that?\"})"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 5}