{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório: Q-learning no Mundo Grade\n",
    "\n",
    "Este notebook explora o algoritmo **Q-learning** em um ambiente do tipo *Grid World*.\n",
    "O objetivo é entender como a definição de recompensas, a estocasticidade das ações e os parâmetros de aprendizado afetam a política aprendida.\n",
    "\n",
    "O ambiente segue a convenção:\n",
    "- Origem `[1,1]` no canto inferior esquerdo.\n",
    "- `x` representa colunas (esquerda → direita).\n",
    "- `y` representa linhas (baixo → cima)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy (origin=bottom-left; display: top→bottom)\n",
      " →   →   →   +1\n",
      " ↑   X   ↑   -1\n",
      " ↑   →   ↑   ← \n"
     ]
    }
   ],
   "source": [
    "# Código base: Q-learning em um Mundo Grade 3x4\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "C, R = 4, 3\n",
    "START = (1, 1)\n",
    "OBSTACLE = (2, 2)\n",
    "TERM_POS = (4, 3)\n",
    "TERM_NEG = (4, 2)\n",
    "\n",
    "ACTIONS = {\n",
    "    'U': (0, +1),\n",
    "    'D': (0, -1),\n",
    "    'L': (-1, 0),\n",
    "    'R': (+1, 0),\n",
    "}\n",
    "\n",
    "STEP_COST = -0.04\n",
    "INVALID_COST = -0.10\n",
    "\n",
    "def valid_state(s):\n",
    "    x, y = s\n",
    "    return 1 <= x <= C and 1 <= y <= R and s != OBSTACLE\n",
    "\n",
    "def is_terminal(s):\n",
    "    return s in (TERM_POS, TERM_NEG)\n",
    "\n",
    "def reward_for(s):\n",
    "    if s == TERM_POS: return +1.0\n",
    "    if s == TERM_NEG: return -1.0\n",
    "    return STEP_COST\n",
    "\n",
    "def step(s, a):\n",
    "    if is_terminal(s):\n",
    "        return s, 0.0, True\n",
    "    dx, dy = ACTIONS[a]\n",
    "    nxt = (s[0] + dx, s[1] + dy)\n",
    "    if not valid_state(nxt):\n",
    "        return s, INVALID_COST, False\n",
    "    r = reward_for(nxt)\n",
    "    done = is_terminal(nxt)\n",
    "    return nxt, r, done\n",
    "\n",
    "Q = defaultdict(float)\n",
    "\n",
    "def epsilon_greedy(s, eps):\n",
    "    if random.random() < eps:\n",
    "        return random.choice(list(ACTIONS.keys()))\n",
    "    best_a, best_q = None, float('-inf')\n",
    "    for a in ['U','D','L','R']:\n",
    "        q = Q[(s, a)]\n",
    "        if q > best_q:\n",
    "            best_q, best_a = q, a\n",
    "    return best_a\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "episodes = 4000\n",
    "\n",
    "def lin_decay(t, T, a, b):\n",
    "    return a + (b - a) * (t / max(1, T - 1))\n",
    "\n",
    "for ep in range(episodes):\n",
    "    s = START\n",
    "    eps = lin_decay(ep, episodes, 0.9, 0.05)\n",
    "    while True:\n",
    "        a = epsilon_greedy(s, eps)\n",
    "        s2, r, done = step(s, a)\n",
    "        if done:\n",
    "            target = r\n",
    "        else:\n",
    "            max_q_next = max(Q[(s2, a2)] for a2 in ACTIONS)\n",
    "            target = r + gamma * max_q_next\n",
    "        Q[(s, a)] += alpha * (target - Q[(s, a)])\n",
    "        s = s2\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "def best_action(s):\n",
    "    if not valid_state(s): return 'X'\n",
    "    if s == TERM_POS: return '+1'\n",
    "    if s == TERM_NEG: return '-1'\n",
    "    a = max(ACTIONS, key=lambda act: Q[(s, act)])\n",
    "    return {'U':'↑','D':'↓','L':'←','R':'→'}[a]\n",
    "\n",
    "print(\"Optimal policy (origin=bottom-left; display: top→bottom)\")\n",
    "for y in range(R, 0, -1):        # topo (y=R) primeiro\n",
    "    row = []\n",
    "    for x in range(1, C + 1):\n",
    "        s = (x, y)\n",
    "        if s == OBSTACLE:\n",
    "            row.append(\" X \")\n",
    "        elif s == TERM_POS:\n",
    "            row.append(\" +1\")\n",
    "        elif s == TERM_NEG:\n",
    "            row.append(\" -1\")\n",
    "        else:\n",
    "            row.append(f\"{best_action(s):^3}\")\n",
    "    print(\" \".join(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38680fc0",
   "metadata": {},
   "source": [
    "# Exercícios\n",
    "\n",
    "### **Exercício 1 – Alterar recompensas**\n",
    "\n",
    "Modifique a função `reward_for()` para investigar como a política muda:\n",
    "\n",
    "1. Aumente o custo de passo (`STEP_COST`) para `-0.2`.\n",
    "2. Diminua o custo de bater na parede (`INVALID_COST`) para `-0.01`.\n",
    "3. Compare as políticas resultantes e descreva o comportamento qualitativamente.\n",
    "\n",
    "### **Exercício 2 – Introduzir estocasticidade**\n",
    "\n",
    "Na função `step()`, faça com que cada ação tenha 80% de chance de ser executada corretamente e 20% de chance de virar à esquerda ou direita aleatoriamente.\n",
    "Verifique se o agente ainda encontra o terminal positivo.\n",
    "\n",
    "\n",
    "### **Exercício 3 – Ajustar hiperparâmetros**\n",
    "\n",
    "Varie:\n",
    "\n",
    "* taxa de aprendizado `alpha ∈ {0.1, 0.5}`\n",
    "* fator de desconto `gamma ∈ {0.8, 0.99}`\n",
    "* número de episódios `episodes ∈ {1000, 10000}`\n",
    "\n",
    "Observe o impacto no tempo de convergência e na estabilidade da política final.\n",
    "\n",
    "### **Exercício 4 – Visualizar convergência**\n",
    "\n",
    "Adicione um gráfico da soma de recompensas por episódio.\n",
    "Use `matplotlib` e mostre a curva de aprendizado do agente.\n",
    "\n",
    "### **Exercício 5 – Política exploratória**\n",
    "\n",
    "Implemente um decaimento **exponencial** de epsilon em vez do linear usado (`lin_decay`).\n",
    "Compare o número de episódios necessários até a política estabilizar.\n",
    "\n",
    "### **Exercício 6 – Novo terminal**\n",
    "\n",
    "Adicione um novo estado terminal em `[1,3]` com recompensa `+0.5`.\n",
    "Treine novamente e observe se o agente passa a preferir esse novo caminho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1 – Alterar recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: altere STEP_COST e INVALID_COST e reexecute o treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2 – Introduzir estocasticidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: introduza ruído de transição (usar random.random() < 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3 – Ajustar hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: experimente diferentes combinações de alpha, gamma e episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 4 – Visualizar convergência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: registre a soma das recompensas e plote a curva de aprendizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 5 – Política exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: substitua lin_decay por decaimento exponencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 6 – Novo terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: inclua o novo terminal na função reward_for()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcc1734",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
